# -*- coding: utf-8 -*-
"""MLProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wMQn1eGXPr6_5ehWr2ScDtcqjcsI-jNM
"""

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import pandas as pd
import numpy as np
from sklearn.decomposition import PC
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('df.csv')

df.head()

label_encoder = LabelEncoder()
df['artNum'] = label_encoder.fit_transform(df['artist'])
df['albNum'] = label_encoder.fit_transform(df['album'])

df.head()

X = df[['albNum', 'artNum', 'loudness', 'valence', 'energy', 'danceability']].values
y_deci = df['popularity'].values / 10
y = np.floor(y_deci)
print(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42069)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

# Create MLP model
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(X_train.shape[1], 128),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(128, 100),
            nn.LeakyReLU(0.2),
            nn.Linear(100, 64),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3),
            nn.Linear(64,30),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.2),
            nn.Linear(30, 10),
        )

    def forward(self, x):
        return self.layers(x)


model = MLP()

optimizer = optim.Adam(model.parameters(), lr=0.001)
loss_fn = nn.CrossEntropyLoss()

loss_history = []
accuracy_history = []

epochs = 500

for epoch in range(epochs): # Run this for however many epochs you defined
    model.train() # Switch the model into training mode. Everything from here just runs the forward and backward pass
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = loss_fn(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()
    loss_history.append(loss.item()) # Keep track of the loss in the list defined above

    with torch.no_grad():
        preds = torch.argmax(model(X_train_tensor), dim=1)
        acc = accuracy_score(y_train_tensor, preds)
        accuracy_history.append(acc) # Keep track of the accuracy in the list defined above

# Plot loss
plt.figure()
plt.plot(loss_history)
plt.title("Training Loss Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.show()

# Plot accuracy

plt.figure()
plt.plot(accuracy_history)
plt.title("Training Accuracy Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.show()

model.eval() # This switches the model from training mode into evaluation mode
with torch.no_grad():
    test_preds = torch.argmax(model(X_test_tensor), dim=1)
    acc = accuracy_score(y_test_tensor, test_preds)
    cm = confusion_matrix(y_test_tensor, test_preds)

print("Test Accuracy:", acc)
print("Confusion Matrix:", cm)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Plot the results
sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue = df['popularity'])
# [:,0] gets first component for all samples, while [:,1] gets second component for all samples
plt.title("PCA of Spotify")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()

# Get the loadings
loadings = pd.DataFrame(
    pca.components_.T,  # Transpose to get features as rows
    columns=["PC1", "PC2"],
    index=['albNum', 'artNum', 'loudness', 'valence', 'energy', 'danceability'] # Use the original column names from df
)

print("\nPCA Loadings (Feature Contributions):")
print(loadings)

# Plot the loadings
loadings.plot(kind='bar')
plt.title("PCA Feature Contributions to PC1 and PC2")
plt.show()

X2 = df[['albNum', 'artNum', 'loudness']]
y2_deci = df['popularity'].values / 10
y2 = np.floor(y)
print(y2)

X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42069)

scaler2 = StandardScaler()
X2_train = scaler2.fit_transform(X2_train)
X2_test = scaler2.transform(X2_test)

X2_train_tensor = torch.tensor(X2_train, dtype=torch.float32)
y2_train_tensor = torch.tensor(y2_train, dtype=torch.long)
X2_test_tensor = torch.tensor(X2_test, dtype=torch.float32)
y2_test_tensor = torch.tensor(y2_test, dtype=torch.long)

# Create MLP model
class MLP2(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(X2_train.shape[1], 128),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.2),
            nn.Linear(128, 100),
            nn.LeakyReLU(0.2),
            nn.Linear(100, 64),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.2),
            nn.Linear(64,30),
            nn.LeakyReLU(0.2),
            #nn.Dropout(0.2),
            nn.Linear(30, 10),
        )

    def forward(self, x):
        return self.layers(x)


model2 = MLP2()

optimizer2 = optim.Adam(model2.parameters(), lr=0.001)
loss_fn2 = nn.CrossEntropyLoss()

loss_history2 = []
accuracy_history2 = []

epochs2 = 500

for epoch2 in range(epochs2):
  model2.train()
  optimizer2.zero_grad()
  outputs2 = model2(X2_train_tensor)
  loss2 = loss_fn2(outputs2, y2_train_tensor)
  loss2.backward()
  optimizer2.step()
  loss_history2.append(loss2.item())

  with torch.no_grad():
    preds2 = torch.argmax(model2(X2_train_tensor), dim=1)
    acc2 = accuracy_score(y2_train_tensor, preds2)
    accuracy_history2.append(acc2)

plt.figure()
plt.plot(loss_history2)
plt.title("Training Loss Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.show()

plt.figure()
plt.plot(accuracy_history2)
plt.title("Training Accuracy Over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.show()

model.eval() # This switches the model from training mode into evaluation mode
with torch.no_grad():
    test_preds2 = torch.argmax(model2(X2_test_tensor), dim=1)
    acc2 = accuracy_score(y2_test_tensor, test_preds2)
    cm2 = confusion_matrix(y2_test_tensor, test_preds2)

print("Test Accuracy:", acc2)
print("Confusion Matrix:", cm2)